{"cells":[{"cell_type":"markdown","metadata":{"id":"xWKrvyhfVL3e"},"source":["# **Milestone 2**"]},{"cell_type":"markdown","metadata":{"id":"Rzh8pcvmtOS8"},"source":["## **Model Building**\n","\n","1. What we want to predict is the \"Price\". We will use the normalized version 'price_log' for modeling.\n","2. Before we proceed to the model, we'll have to encode categorical features. We will drop categorical features like - Name \n","3. We'll split the data into train and test, to be able to evaluate the model that we build on the train data.\n","4. Build Regression models using train data.\n","5. Evaluate the model performance."]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["#Import required libraries\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","\n","#to ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Remove the limit from the number of displayed columns and rows. It helps to see the entire dataframe while printing it\n","pd.set_option(\"display.max_columns\", None)\n","# pd.set_option('display.max_rows', None)\n","pd.set_option(\"display.max_rows\", 200)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["cars_data = pd.read_csv(\"..\\\\..\\\\Public_Datasets\\\\used_cars_milestone_1.csv\")"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7252 entries, 0 to 7251\n","Data columns (total 18 columns):\n"," #   Column                 Non-Null Count  Dtype  \n","---  ------                 --------------  -----  \n"," 0   Unnamed: 0             7252 non-null   int64  \n"," 1   Name                   7252 non-null   object \n"," 2   Location               7252 non-null   object \n"," 3   Year                   7252 non-null   int64  \n"," 4   Kilometers_Driven      7252 non-null   int64  \n"," 5   Fuel_Type              7252 non-null   object \n"," 6   Transmission           7252 non-null   object \n"," 7   Owner_Type             7252 non-null   object \n"," 8   Mileage                7252 non-null   float64\n"," 9   Engine                 7252 non-null   float64\n"," 10  Power                  7252 non-null   float64\n"," 11  Seats                  7252 non-null   float64\n"," 12  New_price              7252 non-null   float64\n"," 13  Price                  6018 non-null   float64\n"," 14  kilometers_driven_log  7252 non-null   float64\n"," 15  price_log              6018 non-null   float64\n"," 16  Brand                  7252 non-null   object \n"," 17  Model                  7252 non-null   object \n","dtypes: float64(8), int64(3), object(7)\n","memory usage: 1019.9+ KB\n"]}],"source":["# drop observations missing the dependent values \n","cars_data.info()"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["cars_data.dropna(inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"0L-oAMItxLP-"},"source":["### **Split Data**"]},{"cell_type":"markdown","metadata":{"id":"Aat-Ne-ZVL3e"},"source":["<li>Step1: Split the data into X and Y . \n","<li>Step2: Encode the categorical variables in X using pd.dummies.\n","<li>Step3: Split the data into train and test using train_test_split."]},{"cell_type":"markdown","metadata":{"id":"Cwh_IhfqVL3f"},"source":["<b>Think about it:</b> Why we should drop 'Name','Price','price_log','Kilometers_Driven' from X before splitting?"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"NTly1jIxtOS8"},"outputs":[],"source":["# Step-1\n","y = cars_data[[\"price_log\", \"Price\"]]\n","X = cars_data.drop(['Unnamed: 0', 'Model', 'Name', 'Price', 'price_log', 'Kilometers_Driven'],axis=1)\n"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"vzCLGMzbVL3f"},"outputs":[],"source":["# Step-2 Use pd.get_dummies(drop_first=True)\n","X = pd.get_dummies(X, drop_first=True)"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"JqVHLEHVRRKK"},"outputs":[{"name":"stdout","output_type":"stream","text":["(4212, 55) (1806, 55)\n"]}],"source":["# Step-3 Splitting data into training and test set:\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['Year', 'Mileage', 'Engine', 'Power', 'Seats', 'New_price',\n","       'kilometers_driven_log', 'Location_Bangalore', 'Location_Chennai',\n","       'Location_Coimbatore', 'Location_Delhi', 'Location_Hyderabad',\n","       'Location_Jaipur', 'Location_Kochi', 'Location_Kolkata',\n","       'Location_Mumbai', 'Location_Pune', 'Fuel_Type_Diesel',\n","       'Fuel_Type_Electric', 'Fuel_Type_LPG', 'Fuel_Type_Petrol',\n","       'Transmission_Manual', 'Owner_Type_Fourth & Above', 'Owner_Type_Second',\n","       'Owner_Type_Third', 'Brand_Audi', 'Brand_BMW', 'Brand_Bentley',\n","       'Brand_Chevrolet', 'Brand_Datsun', 'Brand_Fiat', 'Brand_Force',\n","       'Brand_Ford', 'Brand_Honda', 'Brand_Hyundai', 'Brand_ISUZU',\n","       'Brand_Isuzu', 'Brand_Jaguar', 'Brand_Jeep', 'Brand_Lamborghini',\n","       'Brand_Land', 'Brand_Mahindra', 'Brand_Maruti', 'Brand_Mercedes-Benz',\n","       'Brand_Mini', 'Brand_Mitsubishi', 'Brand_Nissan', 'Brand_Porsche',\n","       'Brand_Renault', 'Brand_Skoda', 'Brand_Smart', 'Brand_Tata',\n","       'Brand_Toyota', 'Brand_Volkswagen', 'Brand_Volvo'],\n","      dtype='object')"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["X_train.columns"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"glAH2vMFtOS8"},"outputs":[],"source":["# Let us write a function for calculating r2_score and RMSE on train and test data.\n","# This function takes model as an input on which we have trained particular algorithm.\n","# the categorical column as the input and returns the boxplots and histograms for the variable.\n","def get_model_score(model, flag=True):\n","    '''\n","    model : classifier to predict values of X\n","\n","    '''\n","    # defining an empty list to store train and test results\n","    score_list=[] \n","    \n","    pred_train = model.predict(X_train)\n","    pred_train_ = np.exp(pred_train)\n","    pred_test = model.predict(X_test)\n","    pred_test_ = np.exp(pred_test)\n","    \n","    train_r2=metrics.r2_score(y_train['Price'],pred_train_)\n","    test_r2=metrics.r2_score(y_test['Price'],pred_test_)\n","    train_rmse=metrics.mean_squared_error(y_train['Price'],pred_train_,squared=False)\n","    test_rmse=metrics.mean_squared_error(y_test['Price'],pred_test_,squared=False)\n","    \n","    #Adding all scores in the list\n","    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))\n","    \n","    # If the flag is set to True then only the following print statements will be dispayed, the default value is True\n","    if flag==True: \n","        print(\"R-square on training set : \",metrics.r2_score(y_train['Price'],pred_train_))\n","        print(\"R-square on test set : \",metrics.r2_score(y_test['Price'],pred_test_))\n","        print(\"RMSE on training set : \",np.sqrt(metrics.mean_squared_error(y_train['Price'],pred_train_)))\n","        print(\"RMSE on test set : \",np.sqrt(metrics.mean_squared_error(y_test['Price'],pred_test_)))\n","    \n","    # returning the list with train and test scores\n","    return score_list"]},{"cell_type":"markdown","metadata":{"id":"o8qcI692VL3g"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"gaj2riZFVL3g"},"source":["For Regression Problems, some of the algorithms used are :<br>\n","\n","**1) Linear Regression** <br>\n","**2) Ridge / Lasso Regression** <br>\n","**3) Decision Trees** <br>\n","**4) Random Forest** <br>"]},{"cell_type":"markdown","metadata":{"id":"xwL33RaztOS9"},"source":["### **Fitting a linear model**"]},{"cell_type":"markdown","metadata":{"id":"kXj-84YCVL3h"},"source":["Linear Regression can be implemented using: <br>\n","\n","**1) Sklearn:** https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html <br>\n","**2) Statsmodels:** https://www.statsmodels.org/stable/regression.html"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"tABeKbbNVL3h"},"outputs":[],"source":["# import Linear Regression from sklearn\n","from sklearn.linear_model import LinearRegression"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"FT3gcKDetOS9"},"outputs":[],"source":["# Create a linear regression model\n","lr = LinearRegression()       "]},{"cell_type":"code","execution_count":73,"metadata":{"id":"rMmX-FJatOS9"},"outputs":[{"data":{"text/plain":["LinearRegression()"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["# Fit linear regression model\n","lr.fit(X_train, y_train['price_log']) "]},{"cell_type":"code","execution_count":76,"metadata":{"id":"ABshmMPAtOS9"},"outputs":[{"name":"stdout","output_type":"stream","text":["R-square on training set :  0.8613929869989658\n","R-square on test set :  0.8607541049366532\n","RMSE on training set :  4.159526260224498\n","RMSE on test set :  4.1588249220448565\n"]}],"source":["# Get score of the model.\n","LR_score = get_model_score(lr)"]},{"cell_type":"markdown","metadata":{"id":"NgNkZ0HctOS9"},"source":["#### **Observations from results: _____**"]},{"cell_type":"markdown","metadata":{"id":"RNN1nlPqIrdC"},"source":["#### **Important variables of Linear Regression**"]},{"cell_type":"markdown","metadata":{"id":"NyKxdVyhVL3i"},"source":["Building a model using statsmodels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHyqhuVMIrdC","scrolled":true},"outputs":[],"source":["# Import Statsmodels \n","\n","# Add constant for test and train\n","x_train = __________\n","\n","# Add constant to test data\n","x_test = ___________\n","\n","def build_ols_model(train):\n","    # Create the model\n","    olsmodel = sm.OLS(y_train[\"price_log\"], train)\n","    return olsmodel.fit()\n","\n","\n","# Fit linear model on new dataset\n","olsmodel1 = build_ols_model(x_train)\n","print(olsmodel1.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C01l5EucVL3i"},"outputs":[],"source":["# Retrive Coeff values, p-values and store them in the dataframe\n","olsmod = pd.DataFrame(olsmodel1.params, columns=['coef'])\n","olsmod['pval']=olsmodel1.pvalues"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbQQZacV9WMm"},"outputs":[],"source":["# FIlter by significant p-value (pval <0.05) and sort descending by Odds ratio\n","olsmod = olsmod.sort_values(by=\"pval\", ascending=False)\n","pval_filter = olsmod['pval']<=0.05\n","olsmod[pval_filter]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjplRhssIrdC"},"outputs":[],"source":["# we are looking are overall significant varaible\n","pval_filter = olsmod['pval']<=0.05\n","imp_vars = olsmod[pval_filter].index.tolist()\n","\n","# we are going to get overall varaibles (un-one-hot encoded varables) from categorical varaibles\n","sig_var = []\n","for col in imp_vars:\n","    if '' in col:\n","        first_part = col.split('_')[0]\n","        for c in data.columns:\n","            if first_part in c and c not in sig_var :\n","                sig_var.append(c)\n"," \n","\n","start = '\\033[1m'\n","end = '\\033[95m'\n","print(start+'Most overall significant categorical varaibles of LINEAR REGRESSION  are '+end,':\\n',sig_var)"]},{"cell_type":"markdown","metadata":{"id":"5uubmKLlVL3j"},"source":["<b>Build Ridge / Lasso Regression similar to Linear Regression:</b><br>\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9--bgDNhVL3j"},"outputs":[],"source":["# import Ridge/ Lasso Regression from sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwSuk8Q9VL3j"},"outputs":[],"source":["# Create a Ridge regression model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cWh1QK0VL3j"},"outputs":[],"source":["# Fit Ridge regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2tIACYOVL3j"},"outputs":[],"source":["# Get score of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUB9svHjVL3j"},"outputs":[],"source":["# Observations"]},{"cell_type":"markdown","metadata":{"id":"owyg5IpstOS9"},"source":["### **Decision Tree** \n","\n","https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjZeRER6VL3k"},"outputs":[],"source":["# import Decision tree for Regression from sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"shBet9WztOS-"},"outputs":[],"source":["# Create a decision tree regression model\n","dtree = _____(random_state=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJxVYVXXtOS-"},"outputs":[],"source":["# Fit decision tree regression model.\n","dtree.fit(_______,_______)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGbEjda0tOS-"},"outputs":[],"source":["# Get score of the model.\n","Dtree_model = get_model_score(_____)"]},{"cell_type":"markdown","metadata":{"id":"UrCgLVKwtOS-"},"source":["#### **Observations from results: _____**"]},{"cell_type":"markdown","metadata":{"id":"6z2V0IwtVL3k"},"source":["Print the importance of features in the tree building ( The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8ro_i9vIrdF"},"outputs":[],"source":["print(pd.DataFrame(dtree.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"]},{"cell_type":"markdown","metadata":{"id":"m9P0pzHHIrdG"},"source":["#### **Observations and insights: _____**"]},{"cell_type":"markdown","metadata":{"id":"q8eFynaNtOS-"},"source":["### **Random Forest**\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jhw-0FsNVL3l"},"outputs":[],"source":["# import Randomforest for Regression from sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4S4FoDXtOS-"},"outputs":[],"source":["# Create a Randomforest regression model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBlavhMTtOS-"},"outputs":[],"source":["# Fit Randomforest regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLDDeeAGtOS_"},"outputs":[],"source":["# Get score of the model."]},{"cell_type":"markdown","metadata":{"id":"ZGNTRfaitOS_"},"source":["#### **Observations and insights: _____**"]},{"cell_type":"markdown","metadata":{"id":"pgwyNxUuIrdG"},"source":["**Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWRS7zISIrdG"},"outputs":[],"source":["# Print important features similar to decision trees"]},{"cell_type":"markdown","metadata":{"id":"cG9ZD9ozIrdH"},"source":["#### **Observations and insights: _____**"]},{"cell_type":"markdown","metadata":{"id":"sw0dMSgetOS_"},"source":["### **Hyperparameter Tuning: Decision Tree**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4eF0aYHHtOS_"},"outputs":[],"source":["# Choose the type of classifier. \n","dtree_tuned = __________(random_state=1)\n","\n","# Grid of parameters to choose from.\n","# Check documentation for all the parametrs that the model takes and play with those.\n","parameters = {________________}\n","\n","# Type of scoring used to compare parameter combinations\n","scorer = _________\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(_____________)\n","grid_obj = grid_obj.fit(______________)\n","\n","# Set the clf to the best combination of parameters\n","dtree_tuned = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data. \n","dtree_tuned.fit(____,____)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hctfJIAXtOS_"},"outputs":[],"source":["# Get score of the dtree_tuned"]},{"cell_type":"markdown","metadata":{"id":"UsGmvq1StOS_"},"source":["#### **Observations and insights: _____**"]},{"cell_type":"markdown","metadata":{"id":"5jssEF5eIrdH"},"source":["**Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdzQWq8WtOTA"},"outputs":[],"source":["# Print important features of tuned decision tree similar to decision trees"]},{"cell_type":"markdown","metadata":{"id":"-r8AR_VotOTB"},"source":["#### **Observations and insights: _____**"]},{"cell_type":"markdown","metadata":{"id":"18uxHTy2tOTB"},"source":["### **Hyperparameter Tuning: Random Forest**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4P_Mj0JYtOTC"},"outputs":[],"source":["# Choose the type of classifier. \n","\n","# Define the parameters for Grid to choose from \n","# Check documentation for all the parametrs that the model takes and play with those\n","\n","# Type of scoring used to compare parameter combinations\n","\n","# Run the grid search\n","\n","# Set the clf to the best combination of parameters\n","\n","# Fit the best algorithm to the data. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSBtYgpctOTC"},"outputs":[],"source":["# Get score of the model."]},{"cell_type":"markdown","metadata":{"id":"a1WHqIX9tOTC"},"source":["#### **Observations and insights: _____**"]},{"cell_type":"markdown","metadata":{"id":"ItsgSUyiIrdI"},"source":["**Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9khvM2ZhtOTC"},"outputs":[],"source":["# Print important features of tuned decision tree similar to decision trees"]},{"cell_type":"markdown","metadata":{"id":"PBoHEXnjtOTC"},"source":["#### **Observations and insights: ______**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCInk4Y8tOTC"},"outputs":[],"source":["# defining list of models ypu have trained\n","models = [lr,dtree, __________________]\n","\n","# defining empty lists to add train and test results\n","r2_train = []\n","r2_test = []\n","rmse_train= []\n","rmse_test= []\n","\n","# looping through all the models to get the rmse and r2 scores\n","for model in models:\n","    # accuracy score\n","    j = get_model_score(model,False)\n","    r2_train.append(j[0])\n","    r2_test.append(j[1])\n","    rmse_train.append(j[2])\n","    rmse_test.append(j[3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuLokC7xtOTD"},"outputs":[],"source":["comparison_frame = pd.DataFrame({'Model':['Linear Regression','Decision Tree',___________,___________], \n","                                          'Train_r2': r2_train,'Test_r2': r2_test,\n","                                          'Train_RMSE':rmse_train,'Test_RMSE':rmse_test}) \n","comparison_frame"]},{"cell_type":"markdown","metadata":{"id":"TZrq2E9VtOTD"},"source":["#### **Observations: _____**"]},{"cell_type":"markdown","metadata":{"id":"acSjU_ZFvyVt"},"source":["**Note:** You can also try some other algorithms such as kNN and compare the model performance with the existing ones"]},{"cell_type":"markdown","metadata":{"id":"58KMVhO_tOTD"},"source":["### **Insights**\n","\n","####**Refined insights**:\n","- What are the most meaningful insights from the data relevant to the problem?\n","\n","####**Comparison of various techniques and their relative performance**:\n","- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?\n","\n","####**Proposal for the final solution design**:\n","- What model do you propose to be adopted? Why is this the best solution to adopt?\n","\n","####**Key recommendations for implementation**: \n","- What are some key recommendations to implement the solutions? What should the implementation roadmap look like? What further analysis needs to be done or what other associated problems need to be solved?"]}],"metadata":{"colab":{"collapsed_sections":["xwL33RaztOS9","owyg5IpstOS9","q8eFynaNtOS-","sw0dMSgetOS_","18uxHTy2tOTB","a1WHqIX9tOTC","PBoHEXnjtOTC","TZrq2E9VtOTD"],"name":"Reference_Notebook_Milestone_2_Regression.ipynb","provenance":[]},"interpreter":{"hash":"7a2ea7a143d669c5ef739d9f587eef65818d88410b58b7a3b0e8ba85fcc16359"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
